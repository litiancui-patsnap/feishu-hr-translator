# LLM Translator MCP Server

> AI-powered content translation server for converting technical reports to HR-friendly language

[![Version](https://img.shields.io/badge/version-0.1.0-blue.svg)](https://github.com/your-org/feishu-hr-translator)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

## ğŸ“– Overview

The LLM Translator MCP Server provides AI-powered capabilities to translate technical content into plain language suitable for non-technical audiences (HR, executives, board members). It wraps the Qwen LLM to offer:

- **Technical â†’ HR Language Translation**: Convert technical jargon to business-friendly terms
- **Risk Extraction**: Identify and assess risks from reports
- **OKR Alignment**: Match work against objectives and key results
- **Action Generation**: Create concrete next steps from reports

## ğŸ¯ Features

### MCP Tools

| Tool | Description | Input | Output |
|------|-------------|-------|--------|
| `translate_to_hr_language` | Main translation function | Technical text + context | HR-friendly summary + risks + OKR alignment |
| `extract_risks` | Extract risks from text | Report text | List of risks with likelihood |
| `infer_okr_alignment` | Match work to OKRs | Report + OKR context | Hit objectives, gaps, confidence |
| `generate_next_actions` | Generate action items | Report text | List of next steps |

### MCP Resources

| Resource | Description |
|----------|-------------|
| `supported_models` | List of available LLM models |
| `prompt_templates` | Prompt templates used for translation |
| `translation_glossary` | Technical terms â†’ plain language mappings |

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/your-org/feishu-hr-translator
cd feishu-hr-translator

# Install dependencies
pip install -r requirements.txt

# Or install as package
cd mcp_servers
pip install -e .
```

### 2. Configuration

Set environment variables:

```bash
# Required
export DASHSCOPE_API_KEY="your_api_key_here"

# Optional
export QWEN_MODEL="qwen-plus"  # or qwen-max, qwen-turbo
export QWEN_API_MODE="text"    # or compatible
export REQUEST_TIMEOUT_SECONDS="30"
```

Or use `.env` file:

```ini
DASHSCOPE_API_KEY=sk-xxxxx
QWEN_MODEL=qwen-plus
QWEN_API_MODE=text
REQUEST_TIMEOUT_SECONDS=30
```

### 3. Test the Server

```bash
# Run standalone test
python -m mcp_servers.llm_translator
```

**Expected output:**
```
ğŸ”„ æµ‹è¯• LLM Translator Server...
è¾“å…¥æ–‡æœ¬: æœ¬å‘¨å®Œæˆäº† WebApp çš„ TDD/BDD æµ‹è¯•æ¡†æ¶æ­å»º...

âœ… ç¿»è¯‘ç»“æœ:
ğŸ“ HR æ€»ç»“: æœ¬å‘¨æ­å»ºäº†ç½‘é¡µç«¯çš„è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·ï¼ˆè®©ç¨‹åºè‡ªåŠ¨æ£€æµ‹é”™è¯¯ï¼‰...

âš ï¸ é£é™©é¡¹:
  - [medium] æ€§èƒ½ä¼˜åŒ–å°šæœªå¼€å§‹ï¼Œå¯èƒ½å½±å“ç”¨æˆ·ä½“éªŒ
    ç¼“è§£æªæ–½: ä¸‹å‘¨ä¼˜å…ˆå®‰æ’æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–å·¥ä½œ

ğŸ¯ å·²å®Œæˆçš„å…³é”®æˆæœ:
  - å®Œæˆè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·å¼€å‘

ğŸ“Œ å¾…æ¨è¿›çš„ç›®æ ‡:
  - æ€§èƒ½ä¼˜åŒ–å·¥ä½œè¿›å±•è¾ƒæ…¢

â¡ï¸ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:
  - å¼€å±•æ€§èƒ½æµ‹è¯•å¹¶åˆ¶å®šä¼˜åŒ–æ–¹æ¡ˆ
  - è¡¥å……æµ‹è¯•ç”¨ä¾‹è¦†ç›–

ğŸ“Š é£é™©ç­‰çº§: medium
ğŸ” OKR å¯¹é½ç½®ä¿¡åº¦: 75%
```

## ğŸ’» Usage Examples

### Example 1: Basic Translation

```python
from mcp_servers.llm_translator import LLMTranslatorServer

# Initialize server
server = LLMTranslatorServer(
    api_key="your_api_key",
    model="qwen-plus"
)

# Translate technical report
result = await server.translate_to_hr_language(
    text="æœ¬å‘¨å®Œæˆäº† API é‡æ„ï¼Œä¼˜åŒ–äº†æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œä¿®å¤äº† 3 ä¸ª P0 bug",
    user_name="å¼ ä¸‰",
    period_type="weekly",
    okr_context="Q1 ç›®æ ‡ï¼šæå‡ç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§"
)

print(result["summary"])
# Output: "æœ¬å‘¨å®Œæˆäº†åå°æ¥å£ä¼˜åŒ–ï¼Œæå‡äº†æ•°æ®æŸ¥è¯¢é€Ÿåº¦ï¼Œè§£å†³äº† 3 ä¸ªä¸¥é‡é—®é¢˜"
```

### Example 2: Risk Extraction

```python
# Extract risks only
risks = await server.extract_risks(
    text="é¡¹ç›®è¿›åº¦å»¶æœŸ 2 å‘¨ï¼Œä¾èµ–çš„ç¬¬ä¸‰æ–¹ API é¢‘ç¹è¶…æ—¶ï¼Œå›¢é˜ŸäººåŠ›ä¸è¶³",
    context="å…³é”®é¡¹ç›®ï¼Œéœ€æŒ‰æ—¶äº¤ä»˜"
)

for risk in risks:
    print(f"[{risk['likelihood']}] {risk['item']}")
    if risk['mitigation']:
        print(f"  â†’ {risk['mitigation']}")

# Output:
# [high] é¡¹ç›®è¿›åº¦å»¶æœŸå¯èƒ½å¯¼è‡´æ— æ³•æŒ‰æ—¶äº¤ä»˜
#   â†’ å¢åŠ äººåŠ›æŠ•å…¥ï¼Œä¼˜åŒ–å…³é”®è·¯å¾„ä»»åŠ¡
# [high] ç¬¬ä¸‰æ–¹æœåŠ¡ä¸ç¨³å®šå½±å“ç³»ç»Ÿå¯ç”¨æ€§
#   â†’ å®æ–½é‡è¯•æœºåˆ¶å’Œé™çº§æ–¹æ¡ˆ
```

### Example 3: OKR Alignment

```python
# Check OKR alignment
alignment = await server.infer_okr_alignment(
    text="å®Œæˆäº†ç”¨æˆ·ç™»å½•ã€æ³¨å†ŒåŠŸèƒ½å¼€å‘ï¼Œå¯†ç é‡ç½®åŠŸèƒ½è¿›åº¦ 50%",
    okr_context="Q1 OKRï¼šå®Œæˆç”¨æˆ·è®¤è¯ç³»ç»Ÿ (KR1: ç™»å½•, KR2: æ³¨å†Œ, KR3: å¯†ç é‡ç½®)"
)

print(f"âœ… å·²å®Œæˆ: {', '.join(alignment['hit_krs'])}")
print(f"ğŸ“Œ å¾…å®Œæˆ: {', '.join(alignment['gaps'])}")
print(f"ğŸ” ç½®ä¿¡åº¦: {alignment['confidence']:.0%}")

# Output:
# âœ… å·²å®Œæˆ: ç”¨æˆ·ç™»å½•åŠŸèƒ½, ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½
# ğŸ“Œ å¾…å®Œæˆ: å¯†ç é‡ç½®åŠŸèƒ½è¿›å±•è¾ƒæ…¢
# ğŸ” ç½®ä¿¡åº¦: 67%
```

### Example 4: Action Generation

```python
# Generate next actions
actions = await server.generate_next_actions(
    text="å®Œæˆäº†å‰ç«¯é¡µé¢å¼€å‘ï¼Œä½†åç«¯æ¥å£è¿˜åœ¨å¼€å‘ä¸­ï¼Œé¢„è®¡ä¸‹å‘¨å®Œæˆ",
    context="éœ€è¦åœ¨æœˆåº•å‰ä¸Šçº¿"
)

for i, action in enumerate(actions, 1):
    print(f"{i}. {action}")

# Output:
# 1. åŠ å¿«åç«¯æ¥å£å¼€å‘è¿›åº¦ï¼Œç¡®ä¿ä¸‹å‘¨å®Œæˆ
# 2. å‰åç«¯è”è°ƒæµ‹è¯•ï¼ŒéªŒè¯åŠŸèƒ½å®Œæ•´æ€§
# 3. å‡†å¤‡ä¸Šçº¿æ–¹æ¡ˆå’Œå›æ»šé¢„æ¡ˆ
```

## ğŸ”§ Claude Desktop Integration

### Step 1: Configure MCP Server

Edit Claude Desktop configuration file:

**macOS/Linux**: `~/.config/claude/mcp_settings.json`
**Windows**: `%APPDATA%\Claude\mcp_settings.json`

```json
{
  "mcpServers": {
    "llm-translator": {
      "command": "python",
      "args": [
        "-m",
        "mcp_servers.llm_translator"
      ],
      "env": {
        "DASHSCOPE_API_KEY": "your_api_key_here",
        "QWEN_MODEL": "qwen-plus"
      }
    }
  }
}
```

### Step 2: Restart Claude Desktop

### Step 3: Use in Claude

```
You: å¸®æˆ‘æŠŠè¿™æ®µæŠ€æœ¯å‘¨æŠ¥ç¿»è¯‘æˆ HR èƒ½çœ‹æ‡‚çš„è¯­è¨€ï¼š
"æœ¬å‘¨å®Œæˆäº† gRPC æœåŠ¡è¿ç§»ï¼Œå®ç°äº† Redis ç¼“å­˜ä¼˜åŒ–ï¼Œé›†æˆäº† K8s è‡ªåŠ¨æ‰©ç¼©å®¹"

Claude: æˆ‘ä¼šä½¿ç”¨ llm-translator MCP Server æ¥ç¿»è¯‘...

[Claude calls: translate_to_hr_language]

Claude: ç¿»è¯‘ç»“æœï¼š
"æœ¬å‘¨å®Œæˆäº†æœåŠ¡é€šä¿¡æ–¹å¼å‡çº§ï¼Œä¼˜åŒ–äº†æ•°æ®ä¸´æ—¶å­˜å‚¨ç³»ç»Ÿï¼ˆæå‡è®¿é—®é€Ÿåº¦ï¼‰ï¼Œå®ç°äº†æœåŠ¡å™¨èµ„æºè‡ªåŠ¨è°ƒæ•´ï¼ˆé«˜å³°æœŸè‡ªåŠ¨æ‰©å®¹ï¼‰"
```

## ğŸ“Š Supported Models

| Model | Provider | Cost | Best For | Context |
|-------|----------|------|----------|---------|
| `qwen-max` | DashScope | Higher | Highest quality | 8K tokens |
| `qwen-plus` | DashScope | Medium | **Recommended** balance | 32K tokens |
| `qwen-turbo` | DashScope | Lower | Fast simple tasks | 8K tokens |
| `qwen-long` | DashScope | Medium | Long documents | 10M tokens |

Get full list via:
```python
models = server.get_supported_models()
```

## ğŸ¨ Translation Glossary

The server uses an extensive glossary to translate technical terms:

| Technical | Plain Language |
|-----------|----------------|
| API | ç³»ç»Ÿæ¥å£ / ç¨‹åºè¿æ¥é€šé“ |
| SDK | å¼€å‘å·¥å…·åŒ… |
| TDD/BDD | æµ‹è¯•é©±åŠ¨å¼€å‘æ–¹æ³• |
| CI/CD | è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹ |
| é‡æ„ | ä»£ç ä¼˜åŒ– |
| å¼‚æ­¥ | åå°å¤„ç† / ä¸é˜»å¡æ“ä½œ |
| Bug | ç¨‹åºé”™è¯¯ |
| å‰ç«¯/åç«¯ | ç”¨æˆ·ç•Œé¢ / æœåŠ¡å™¨ç¨‹åº |

Get full glossary via:
```python
glossary = server.get_translation_glossary()
```

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/mcp_tests/test_llm_translator.py

# Run with coverage
pytest --cov=mcp_servers tests/

# Run integration test
python -m mcp_servers.llm_translator
```

## ğŸ“ˆ Performance

- **Average latency**: 2-5 seconds (depends on model)
- **Max input**: 8K tokens (qwen-max/turbo), 32K tokens (qwen-plus), 10M tokens (qwen-long)
- **Output**: ~200-300 words summary + structured data
- **Cost**: ~Â¥0.002-0.01 per translation (DashScope pricing)

## ğŸ”’ Security & Privacy

- **API Keys**: Never logged or stored, only used for API calls
- **Data**: Input text sent to DashScope API (é˜¿é‡Œäº‘), review their privacy policy
- **Caching**: No caching of API responses by default
- **Logging**: Only logs errors and metrics, not content

## ğŸ› ï¸ Advanced Configuration

### Custom Prompt Templates

```python
# Customize prompts
from src.ai.qwen import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE

# Modify templates in src/ai/qwen.py or override at runtime
```

### Retry & Timeout

```python
server = LLMTranslatorServer(
    api_key="...",
    timeout=60.0,  # Increase timeout for long documents
)

# Retry logic is built-in (max 2 retries with exponential backoff)
```

### Fallback Behavior

If API fails, the server returns:
- Summary: First 180 chars of original text + "(ç¦»çº¿æ¨¡å¼)"
- Risks: Empty list
- OKR: Default low-confidence alignment
- Actions: Generic suggestions based on period type

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repo
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

MIT License - see [LICENSE](../LICENSE) file for details

## ğŸ™ Acknowledgments

- Built on [Qwen](https://help.aliyun.com/zh/dashscope/) by Alibaba Cloud
- Part of [Feishu HR Translator](https://github.com/your-org/feishu-hr-translator) project
- Inspired by [Model Context Protocol](https://modelcontextprotocol.io/)

## ğŸ“ Support

- Issues: [GitHub Issues](https://github.com/your-org/feishu-hr-translator/issues)
- Discussions: [GitHub Discussions](https://github.com/your-org/feishu-hr-translator/discussions)
- Email: support@your-org.com

---

**Made with â¤ï¸ for HR teams who deserve plain language**
